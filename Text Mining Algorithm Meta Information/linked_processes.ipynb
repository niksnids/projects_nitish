{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"linked_processes.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPUy92SUCqpX8qDlFkPzwSz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"LZ-kdkDUNSRg"},"source":["!pip install tika"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTItnwzRNTs4"},"source":["from tika import parser\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2e4riD5NZRC"},"source":["def find_linked_processes(input_document):\n","\n","  #link to input document\n","  input = input_document\n","\n","  #convert pdf to python\n","  appsani = parser.from_file(input)\n","\n","  #look for term related documents\n","  linkeddocs = re.findall(r'(.+RELATED DOCUMENTS[^\\n]+\\n)', appsani[\"content\"], flags = re.IGNORECASE)\n","\n","  #convert first entry of the list to string\n","  string = str(linkeddocs[0])\n","\n","  #get all lines after the string\n","  relateddocs = appsani[\"content\"][appsani[\"content\"].find(string):]\n","\n","  #as there seems to be some odd whitespaces, I remove all whitespaces\n","  relateddocsnospaces = relateddocs.replace(\" \", \"\")\n","\n","  #I split the string for every \\n, one time without white spaces and one time with white spaces\n","  relateddocssplitnospaces = relateddocsnospaces.split(\"\\n\")\n","  relateddocssplit = relateddocs.split(\"\\n\")\n","\n","  #I check for all linked process documents based on when there are more linbreaks than 1\n","  for line in range(len(relateddocssplitnospaces)):\n","    if relateddocssplitnospaces[line] == \"\" and relateddocssplitnospaces[line+1] == \"\":\n","      position = line\n","      break\n","    else:\n","      pass\n","\n","  #I split the list on the above solution\n","  finalrelateddocs = relateddocssplit[0:position]\n","\n","  #I remove empty strings\n","  finallist = list(filter(None, finalrelateddocs))\n","\n","  #return finallist\n","  return finallist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lcZ__WqONiGL"},"source":["def find_linked_processes1(input_document):\n","  input = input_document\n","\n","  #convert pdf to python\n","  lsbu = parser.from_file(input)\n","\n","  #Find all sentences which include for more information\n","  fmi = re.findall(r'([^.]*For more information[^.]+.)', lsbu[\"content\"], flags = re.IGNORECASE)\n","\n","  #remove line breaks\n","  fmiclean = []\n","  for element in fmi:\n","      fmiclean.append(element.replace(\"\\n\",\"\"))\n","  return fmiclean"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nz421b9mNij-"},"source":["def comb_linked_processes(input_document):\n","  input = input_document\n","  \n","  #use parser\n","  inputconv = parser.from_file(input)\n","\n","  #check for the different extraction methods\n","  related = re.findall(r'(.+RELATED DOCUMENTS[^\\n]+\\n)', inputconv[\"content\"], flags = re.IGNORECASE)\n","  fmi = re.findall(r'([^.]*For more information[^.]+.)', inputconv[\"content\"], flags = re.IGNORECASE)\n","\n","  #depending on the outcome, use the respective function\n","  if related:\n","    return find_linked_processes(input)\n","  elif fmi:\n","    return find_linked_processes1(input)\n","  else:\n","    pass"],"execution_count":null,"outputs":[]}]}