# -*- coding: utf-8 -*-
"""Copy of 2021-06-06 Actors_Departments.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rtxa7GoVGRvu081bEnWNzFrL3uq-Olnn

# Import stuff
"""

!pip install pprintpp

!pip install tika

from tika import parser
import re
from google.colab import drive
drive.mount('/content/gdrive')
from collections import Counter
import spacy
from spacy import displacy
from gensim.parsing.preprocessing import STOPWORDS, strip_tags, strip_numeric, strip_punctuation, strip_multiple_whitespaces, remove_stopwords, strip_short, stem_text
from collections import Counter
import en_core_web_sm
import pprintpp
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
nlp = en_core_web_sm.load()
import numpy as np

"""# Actors / Departments"""

#same approach as you had in the main class too, create a list with the files and then just chose the respective one
file_list = [
             '04_SOP-Business-Plan.pdf', #0
             'AccountsPayablePolicy.pdf', #1
             'purchasing-payment-policy.pdf', #2
             'UniversityofMichiganGift-Cards1.pdf' #3
]
file_index = 3
raw = "/content/gdrive/My Drive/Signavio_Test/data/"+file_list[file_index]

#convert pdf and call the dataframe own (I can't recall why I called it like this haha :D)
own = parser.from_file(raw)["content"]
own

#I have created a second df called owncleaned, which I will use to do some preprocessing on
#I did it in case I ruin it, so I can always go back to this cell and overwrite all the changes
owncleaned = own
owncleaned

#replace all line breaks with ""
owncleaned = owncleaned.replace("\n","")
#do some text preprocessing
owncleaned = [strip_numeric(data) for data in owncleaned]
owncleaned = [strip_punctuation(data) for data in owncleaned]
owncleaned = [strip_multiple_whitespaces(data) for data in owncleaned]
owncleaned = ("").join(owncleaned)
#remove stopwords
nltk_stopwords = set(stopwords.words('english'))
owncleaned = remove_stopwords(owncleaned)
#show the preprocessed text
owncleaned

#run spacy name entity recognition model on it and store results of organization in mostcommon
doc = nlp(owncleaned)
items = [x.text for x in doc.ents if x.label_ in ["ORG"]]
mostcommon = np.array(Counter(items).most_common())

#return all entities, which appear more often than 2 times
for i, j in mostcommon:
  if int(j) >= 3:
    print(i,j)

#You can leave this out!
#this is actually only for our information and doesn't add any value to the above code. Just intersting to see how many org. and so on were found
#check all labels and the respective counts of the labels
labels = [x.label_ for x in doc.ents]
Counter(labels)